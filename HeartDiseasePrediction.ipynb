{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HeartDiseasePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP5LIqKHH7X-"
      },
      "source": [
        "# Regular Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToUtDnUWIXPz"
      },
      "source": [
        "Get Data and get it ready for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAJ72qjxIWvO"
      },
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "dataset = pd.read_csv(url, delimiter=\",\", header=None)\n",
        "dataset.columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "dataset.where(dataset != '?', np.nan, inplace=True)\n",
        "dataset['target'] = dataset['target'].map({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})\n",
        "null_columns = dataset.isna().sum()\n",
        "for col_name, num_null_values in null_columns.iteritems():\n",
        "  if (num_null_values != 0):\n",
        "    dataset[col_name].fillna(dataset[col_name].mode()[0], inplace=True)\n",
        "X = dataset.iloc[:, :-1]\n",
        "y = dataset.iloc[:, -1]\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ZdFHUWIo7z"
      },
      "source": [
        "MLP Classifier with 10 hidden layers, learning rate 0.1 and 10000 iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvFIxLonI_Oz"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlpc = MLPClassifier(hidden_layer_sizes=(10,), max_iter=10000, learning_rate_init=0.1)\n",
        "pred_mlpc = mlpc.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR99_Ej4JRbI"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmvsysFQJZqo"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier()\n",
        "pred_rfc = rfc.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu2P_4rGJzrP"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hW-z6dzJ3xp"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "nbc = GaussianNB()\n",
        "pred_nbc = nbc.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4fwn62XKIKp"
      },
      "source": [
        "Decison Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8S2bndsKJ_i"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "pred_dtc = dtc.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoSGBRbwKa45"
      },
      "source": [
        "SVM Linear Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRw_6FOwKeJX"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc_l = SVC(kernel=\"linear\", C=10, gamma=0.1)\n",
        "pred_svc_l = svc_l.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIFr5amtKsst"
      },
      "source": [
        "SVM RBF Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PWQUNweKv98"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svc_r = SVC(kernel=\"rbf\", C=10, gamma=0.1)\n",
        "pred_svc_r = svc_r.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYZAOKxPK4UP"
      },
      "source": [
        "Fuzzy kNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY9IRIG6K7Y-"
      },
      "source": [
        "import operator\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "class FuzzyKNN(BaseEstimator, ClassifierMixin):\n",
        "  def __init__(self, k=3, plot=False):\n",
        "    self.k = k\n",
        "    self.plot = plot\n",
        "  def fit(self, X, y=None):\n",
        "    self._check_params(X,y)\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.xdim = len(self.X[0])\n",
        "    self.n = len(y)\n",
        "    classes = list(set(y))\n",
        "    classes.sort()\n",
        "    self.classes = classes\n",
        "    self.df = pd.DataFrame(self.X)\n",
        "    self.df['y'] = self.y\n",
        "    self.memberships = self._compute_memberships()\n",
        "    self.df['membership'] = self.memberships\n",
        "    self.fitted_ = True\n",
        "    return self\n",
        "  def predict(self, X):\n",
        "    if self.fitted_ == None:\n",
        "      raise Exception('predict() called before fit()')\n",
        "    else:\n",
        "      m = 2\n",
        "      y_pred = []\n",
        "      for x in X:\n",
        "        neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "        votes = {}\n",
        "        for c in self.classes:\n",
        "          den = 0\n",
        "          for n in range(self.k):\n",
        "            dist = np.linalg.norm(x - neighbors.iloc[n,0:self.xdim])\n",
        "            den += 1 / (dist ** (2 / (m-1)))\n",
        "          neighbors_votes = []\n",
        "          for n in range(self.k):\n",
        "            dist = np.linalg.norm(x - neighbors.iloc[n,0:self.xdim])\n",
        "            num = (neighbors.iloc[n].membership[c]) / (dist ** (2 / (m-1)))\n",
        "            vote = num/den\n",
        "            neighbors_votes.append(vote)\n",
        "          votes[c] = np.sum(neighbors_votes)\n",
        "        pred = max(votes.items(), key=operator.itemgetter(1))[0]\n",
        "        # y_pred.append((pred, votes))\n",
        "        y_pred.append(pred)\n",
        "      return y_pred\n",
        "  def score(self, X, y):\n",
        "    if self.fitted_ == None:\n",
        "      raise Exception('score() called before fit()')\n",
        "    else:\n",
        "      predictions = self.predict(X)\n",
        "      y_pred = [t[0] for t in predictions]\n",
        "      confidences = [t[1] for t in predictions]\n",
        "      return accuracy_score(y_pred=y_pred, y_true=y)\n",
        "  def _find_k_nearest_neighbors(self, df, x):\n",
        "    X = df.iloc[:,0:self.xdim].values\n",
        "    df['distances'] = [np.linalg.norm(X[i] - x) for i in range(self.n)]\n",
        "    df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "    neighbors = df.iloc[0:self.k]\n",
        "    return neighbors\n",
        "  def _get_counts(self, neighbors):\n",
        "    groups = neighbors.groupby('y')\n",
        "    counts = {group[1]['y'].iloc[0]:group[1].count()[0] for group in groups}\n",
        "    return counts\n",
        "  def _compute_memberships(self):\n",
        "    memberships = []\n",
        "    # for i in range(self.n):\n",
        "    for i, j in zip(self.X, self.y):\n",
        "      # x = self.X[i]\n",
        "      # Y = self.y[i]\n",
        "      x = i\n",
        "      Y = j\n",
        "      neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "      counts = self._get_counts(neighbors)\n",
        "      membership = dict()\n",
        "      for c in self.classes:\n",
        "        try:\n",
        "          uci = 0.49 * (counts[c] / self.k)\n",
        "          if c == Y:\n",
        "            uci += 0.51\n",
        "          membership[c] = uci\n",
        "        except:\n",
        "          membership[c] = 0\n",
        "      memberships.append(membership)\n",
        "    return memberships\n",
        "  def _check_params(self, X, y):\n",
        "    if type(self.k) != int:\n",
        "      raise Exception('\"k\" should have type int')\n",
        "    if self.k >= len(y):\n",
        "      raise Exception('\"k\" should be less than no of feature sets')\n",
        "    if self.k % 2 == 0:\n",
        "      raise Exception('\"k\" should be odd')\n",
        "    if type(self.plot) != bool:\n",
        "      raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "fKNN = FuzzyKNN(k=19)\n",
        "pred_fKNN = fKNN.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELcL3FYaMUF0"
      },
      "source": [
        "Gradint Boosting Decision Tree Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-AxndjnMjaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d9cc73-feb7-4f80-ca0f-18ddc21cf15b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gbc = GradientBoostingClassifier(\n",
        "    n_estimators=10,\n",
        "    max_depth=5,\n",
        "    min_samples_leaf=2,\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "X_Coded = OneHotEncoder().fit_transform(gbc.apply(X)[:, :, 0])\n",
        "X_train_hot_coded, X_test_hot_coded, y_train, y_test = train_test_split(X_Coded, y, test_size=0.2, random_state=42)\n",
        "gbclr = LogisticRegression(penalty='l2', random_state=42).fit(X_train_hot_coded, y_train)\n",
        "pred_gbclr = gbclr.predict(X_test_hot_coded)\n",
        "\n",
        "gbclr_alt = LogisticRegression(penalty='l2', random_state=42).fit(gbc.apply(X_train)[:, :, 0], y_train)\n",
        "pred_gbclr_alt = gbclr_alt.predict(gbc.apply(X_test)[:, :, 0])"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RqAzx1QOXJV"
      },
      "source": [
        "HRFLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsSM-uByOaPh"
      },
      "source": [
        "from copy import deepcopy\n",
        "from sklearn.metrics import mean_squared_error\n",
        "class ModelTree(object):\n",
        "    def __init__(self, model, max_depth=5, min_samples_leaf=10):\n",
        "        self.model = model\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.tree = None\n",
        "    def fit(self, X, y, verbose=False):\n",
        "        # Settings\n",
        "        model = self.model\n",
        "        min_samples_leaf = self.min_samples_leaf\n",
        "        max_depth = self.max_depth\n",
        "        if verbose:\n",
        "            print(\" max_depth={}, min_samples_leaf={}...\".format(max_depth, min_samples_leaf))\n",
        "        def _build_tree(X, y):\n",
        "            global index_node_global\n",
        "            def _create_node(X, y, depth, container):\n",
        "                loss_node, model_node = _fit_model(X, y, model)\n",
        "                node = {\"name\": \"node\",\n",
        "                        \"index\": container[\"index_node_global\"],\n",
        "                        \"loss\": loss_node,\n",
        "                        \"model\": model_node,\n",
        "                        \"data\": (X, y),\n",
        "                        \"n_samples\": len(X),\n",
        "                        \"j_feature\": None,\n",
        "                        \"threshold\": None,\n",
        "                        \"children\": {\"left\": None, \"right\": None},\n",
        "                        \"depth\": depth}\n",
        "                container[\"index_node_global\"] += 1\n",
        "                return node\n",
        "            # Recursively split node + traverse node until a terminal node is reached\n",
        "            def _split_traverse_node(node, container):\n",
        "                # Perform split and collect result\n",
        "                result = _splitter(node, model, max_depth=max_depth,min_samples_leaf=min_samples_leaf)\n",
        "                # Return terminal node if split is not advised\n",
        "                if not result[\"did_split\"]:\n",
        "                    if verbose:\n",
        "                        depth_spacing_str = \" \".join([\" \"] * node[\"depth\"])\n",
        "                        print(\" {}*leaf {} @ depth {}: loss={:.6f}, N={}\".format(depth_spacing_str, node[\"index\"], node[\"depth\"], node[\"loss\"], result[\"N\"]))\n",
        "                    return\n",
        "\n",
        "                # Update node information based on splitting result\n",
        "                node[\"j_feature\"] = result[\"j_feature\"]\n",
        "                node[\"threshold\"] = result[\"threshold\"]\n",
        "                del node[\"data\"]  # delete node stored data\n",
        "\n",
        "                # Extract splitting results\n",
        "                (X_left, y_left), (X_right, y_right) = result[\"data\"]\n",
        "                model_left, model_right = result[\"models\"]\n",
        "\n",
        "                # Report created node to user\n",
        "                if verbose:\n",
        "                    depth_spacing_str = \" \".join([\" \"] * node[\"depth\"])\n",
        "                    print(\" {}node {} @ depth {}: loss={:.6f}, j_feature={}, threshold={:.6f}, N=({},{})\".format(depth_spacing_str, node[\"index\"], node[\"depth\"], node[\"loss\"], node[\"j_feature\"], node[\"threshold\"], len(X_left), len(X_right)))\n",
        "\n",
        "                # Create children nodes\n",
        "                node[\"children\"][\"left\"] = _create_node(X_left, y_left, node[\"depth\"]+1, container)\n",
        "                node[\"children\"][\"right\"] = _create_node(X_right, y_right, node[\"depth\"]+1, container)\n",
        "                node[\"children\"][\"left\"][\"model\"] = model_left\n",
        "                node[\"children\"][\"right\"][\"model\"] = model_right\n",
        "\n",
        "                # Split nodes\n",
        "                _split_traverse_node(node[\"children\"][\"left\"], container)\n",
        "                _split_traverse_node(node[\"children\"][\"right\"], container)\n",
        "\n",
        "            container = {\"index_node_global\": 0}  # mutatable container\n",
        "            root = _create_node(X, y, 0, container)  # depth 0 root node\n",
        "            _split_traverse_node(root, container)  # split and traverse root node\n",
        "\n",
        "            return root\n",
        "\n",
        "        # Construct tree\n",
        "        self.tree = _build_tree(X, y)\n",
        "        return self.tree\n",
        "    # ======================\n",
        "    # Predict\n",
        "    # ======================\n",
        "    def predict(self, X):\n",
        "        assert self.tree is not None\n",
        "        def _predict(node, x):\n",
        "            no_children = node[\"children\"][\"left\"] is None and \\\n",
        "                          node[\"children\"][\"right\"] is None\n",
        "            if no_children:\n",
        "                y_pred_x = node[\"model\"].predict([x])[0]\n",
        "                return y_pred_x\n",
        "            else:\n",
        "                if x[node[\"j_feature\"]] <= node[\"threshold\"]:  # x[j] < threshold\n",
        "                    return _predict(node[\"children\"][\"left\"], x)\n",
        "                else:  # x[j] > threshold\n",
        "                    return _predict(node[\"children\"][\"right\"], x)\n",
        "        y_pred = np.array([_predict(self.tree, x) for x in X])\n",
        "        return y_pred\n",
        "    # ======================\n",
        "    # Loss\n",
        "    # ======================\n",
        "    def loss(self, X, y, y_pred):\n",
        "        loss = self.model.loss(X, y, y_pred)\n",
        "        return loss\n",
        "def _splitter(node, model,max_depth=5, min_samples_leaf=10):\n",
        "    # Extract data\n",
        "    X, y = node[\"data\"]\n",
        "    depth = node[\"depth\"]\n",
        "    N, d = X.shape\n",
        "\n",
        "    # Find feature splits that might improve loss\n",
        "    did_split = False\n",
        "    loss_best = node[\"loss\"]\n",
        "    data_best = None\n",
        "    models_best = None\n",
        "    j_feature_best = None\n",
        "    threshold_best = None\n",
        "\n",
        "    # Perform threshold split search only if node has not hit max depth\n",
        "    if (depth >= 0) and (depth < max_depth):\n",
        "        for j_feature in range(d):\n",
        "            threshold_search = []\n",
        "            for i in range(N):\n",
        "                threshold_search.append(X[i, j_feature])\n",
        "            # Perform threshold split search on j_feature\n",
        "            for threshold in threshold_search:\n",
        "                # Split data based on threshold\n",
        "                (X_left, y_left), (X_right, y_right) = _split_data(j_feature, threshold, X, y)\n",
        "                N_left, N_right = len(X_left), len(X_right)\n",
        "\n",
        "                # Splitting conditions\n",
        "                split_conditions = [N_left >= min_samples_leaf,\n",
        "                                    N_right >= min_samples_leaf]\n",
        "\n",
        "                # Do not attempt to split if split conditions not satisfied\n",
        "                if not all(split_conditions):\n",
        "                    continue\n",
        "\n",
        "                # Compute weight loss function\n",
        "                loss_left, model_left = _fit_model(X_left, y_left, model)\n",
        "                loss_right, model_right = _fit_model(X_right, y_right, model)\n",
        "                loss_split = (N_left*loss_left + N_right*loss_right) / N\n",
        "\n",
        "                # Update best parameters if loss is lower\n",
        "                if loss_split < loss_best:\n",
        "                    did_split = True\n",
        "                    loss_best = loss_split\n",
        "                    models_best = [model_left, model_right]\n",
        "                    data_best = [(X_left, y_left), (X_right, y_right)]\n",
        "                    j_feature_best = j_feature\n",
        "                    threshold_best = threshold\n",
        "\n",
        "    # Return the best result\n",
        "    result = {\"did_split\": did_split,\n",
        "              \"loss\": loss_best,\n",
        "              \"models\": models_best,\n",
        "              \"data\": data_best,\n",
        "              \"j_feature\": j_feature_best,\n",
        "              \"threshold\": threshold_best,\n",
        "              \"N\": N}\n",
        "\n",
        "    return result\n",
        "\n",
        "def _fit_model(X, y, model):\n",
        "    model_copy = deepcopy(model)  # must deepcopy the model!\n",
        "    model_copy.fit(X,y)\n",
        "    y_pred = model_copy.predict(X)\n",
        "    loss = model_copy.loss(X, y, y_pred)\n",
        "    assert loss >= 0.0\n",
        "    return loss, model_copy\n",
        "\n",
        "def _split_data(j_feature, threshold, X, y):\n",
        "    idx_left = np.where(X[:, j_feature] <= threshold)[0]\n",
        "    idx_right = np.delete(np.arange(0, len(X)), idx_left)\n",
        "    assert len(idx_left) + len(idx_right) == len(X)\n",
        "    idx_left_bool = [False for i in X]\n",
        "    idx_right_bool = [False for i in X]\n",
        "    for i in idx_left:\n",
        "      idx_left_bool[i] = True\n",
        "    for i in idx_right:\n",
        "      idx_right_bool[i] = True\n",
        "    return (X[idx_left_bool], y[idx_left_bool]), (X[idx_right_bool], y[idx_right_bool])\n",
        "\n",
        "class logistic_regr:\n",
        "\n",
        "    def __init__(self):\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        self.model = LogisticRegression(penalty=\"l2\",solver='liblinear')\n",
        "        self.flag = False\n",
        "        self.flag_y_pred = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_unique = list(set(y))\n",
        "        if len(y_unique) == 1:\n",
        "            self.flag = True\n",
        "            self.flag_y_pred = y_unique[0]\n",
        "        else:\n",
        "            self.model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.flag:\n",
        "            return self.flag_y_pred * np.ones((len(X),), dtype=int)\n",
        "        else:\n",
        "            return self.model.predict(X)\n",
        "\n",
        "    def loss(self, X, y, y_pred):\n",
        "        return mean_squared_error(y, y_pred)\n",
        "\n",
        "    def predict_proba(self,X):\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "\n",
        "#No of decisioin Trees\n",
        "esitmators=5\n",
        "pred_HRFLM=[]\n",
        "n_train_split=int(len(X_train)/esitmators)\n",
        "inital_train=0\n",
        "final_train=0\n",
        "yy_pred=[]\n",
        "classifier=None\n",
        "for i in range(1,esitmators+1):\n",
        "    classifier =logistic_regr()\n",
        "    final_train=i*n_train_split\n",
        "    temp_X_train=X_train[inital_train:final_train]\n",
        "    temp_y_train=y_train[inital_train:final_train]\n",
        "    L=ModelTree(classifier,max_depth=20, min_samples_leaf=10)\n",
        "    node=L.fit(temp_X_train,temp_y_train,verbose=False)\n",
        "    classifier=node[\"model\"]\n",
        "    y_pred_temp=L.predict(X_test)\n",
        "    yy_pred.append(y_pred_temp)\n",
        "for j in range(len(yy_pred[0])):\n",
        "    curr=[]\n",
        "    for i in range(len(yy_pred)):\n",
        "        curr.append(yy_pred[i][j])\n",
        "    a=curr.count(0)\n",
        "    b=curr.count(1)\n",
        "    if a>b:\n",
        "        pred_HRFLM.append(0)\n",
        "    else:\n",
        "        pred_HRFLM.append(1)\n"
      ],
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyK3cwIsStvT"
      },
      "source": [
        "SVM + Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKShMuZTDku"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "sfm = SelectFromModel(estimator=RandomForestClassifier()).fit(X_train, y_train)\n",
        "X_train_new = sfm.transform(X_train)\n",
        "X_test_new = sfm.transform(X_test)\n",
        "svcrf_r = SVC(kernel=\"rbf\", C=10, gamma=0.1)\n",
        "pred_svcrf_r = svcrf_r.fit(X_train_new, y_train).predict(X_test_new)\n",
        "\n",
        "svcrf_l = SVC(kernel=\"linear\")\n",
        "pred_svcrf_l = svcrf_l.fit(X_train_new, y_train).predict(X_test_new)"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr0hd6PkT7dW"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvnawKTtT8Ui",
        "outputId": "b19f5bf6-530d-474e-a55c-47d9588399f6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Input, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "X_train_reshaped = X_train.reshape(len(X_train), len(X_train[0]), 1)\n",
        "X_test_reshaped = X_test.reshape(len(X_test), len(X_test[0]), 1)\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(13, 1)))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=(2)))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=(1)))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "model.fit(X_train_reshaped, y_train, batch_size=32, epochs=100, verbose=0)\n",
        "loss, acc_CNN = model.evaluate(X_test_reshaped, y_test)\n",
        "# pred_CNN = model.predict(X_test_reshaped).flatten()\n",
        "# pred_CNN = (pred_CNN > 0.5)"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fcb2d14fe60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1920 - accuracy: 0.7172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aSk8lzFnE4Z"
      },
      "source": [
        "LR with Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZuhFDaWnIYx"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "# from sklearn.cluster import KMeans\n",
        "# from copy import deepcopy\n",
        "# km = KMeans(n_clusters=2)\n",
        "# km.fit(X)\n",
        "# distances = km.transform(X)\n",
        "# d1 = distances[:, 0]\n",
        "# d2 = distances[:, 1]\n",
        "# # sorted_idx = np.argsort(distances.ravel())[::-1][:5]\n",
        "# s1 = np.argsort(d1)[::-1][:5]\n",
        "# s2 = np.argsort(d2)[::-1][:5]\n",
        "# s = list(set(np.concatenate([s1, s2])))\n",
        "# t = deepcopy(s)\n",
        "# for i in s:\n",
        "#   print (i, y[i])\n",
        "# new_X = np.delete(X, s, axis=0)\n",
        "# new_y = np.delete(y, t, axis=0)\n",
        "iso = LocalOutlierFactor()\n",
        "yhat = iso.fit_predict(X_train)\n",
        "mask = yhat != -1\n",
        "X_train, y_train = X_train[mask, :], y_train[mask]\n",
        "lr = LogisticRegression()\n",
        "pred_lr = lr.fit(X_train, y_train).predict(X_test)"
      ],
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy87IRTFQvcI"
      },
      "source": [
        "Accuracies of different methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BoIIcOHQy0b",
        "outputId": "35287dcc-3954-4f11-d685-010ad46fba0e"
      },
      "source": [
        "print (\"MLP:                  \", accuracy_score(y_test, pred_mlpc))\n",
        "print (\"Random Forest:        \", accuracy_score(y_test, pred_rfc))\n",
        "print (\"Naive Bayes:          \", accuracy_score(y_test, pred_nbc))\n",
        "print (\"HRFLM:                \", accuracy_score(y_test, pred_HRFLM))\n",
        "print (\"GBCLR:                \", accuracy_score(y_test, pred_gbclr))\n",
        "print (\"GBCLR ALT:            \", accuracy_score(y_test, pred_gbclr_alt))\n",
        "print (\"Decision Tree:        \", accuracy_score(y_test, pred_dtc))\n",
        "print (\"SVM RBF:              \", accuracy_score(y_test, pred_svc_r))\n",
        "print (\"Fuzzy kNN:            \", accuracy_score(y_test, pred_fKNN))\n",
        "print (\"SVMRF (Our Method):   \", accuracy_score(y_test, pred_svcrf_r))\n",
        "print (\"LR Anomaly Detection: \", accuracy_score(y_test, pred_lr))\n",
        "print (\"CNN:                  \", acc_CNN)"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP:                   0.7868852459016393\n",
            "Random Forest:         0.8852459016393442\n",
            "Naive Bayes:           0.8360655737704918\n",
            "HRFLM:                 0.8852459016393442\n",
            "GBCLR:                 0.8360655737704918\n",
            "GBCLR ALT:             0.8360655737704918\n",
            "Decision Tree:         0.7213114754098361\n",
            "SVM Linear:            0.8688524590163934\n",
            "SVM RBF:               0.8688524590163934\n",
            "Fuzzy kNN:             0.9344262295081968\n",
            "SVMRF Linear:          0.8688524590163934\n",
            "SVMRF RBF:             0.9016393442622951\n",
            "LR Anomaly Detection:  0.8852459016393442\n",
            "CNN:                   0.7172130942344666\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}